{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Applications | Addition\n",
    "\n",
    "**Tutorial on Transformers for Mathematics**\n",
    "\n",
    "*Simons Institute and SLMath Joint Workshop: AI for Mathematics and Theoretical Computer Science, April 8 2025*\n",
    "\n",
    "Author: Sean Welleck\n",
    "\n",
    "------\n",
    "\n",
    "This notebook trains a transformer language model on a dataset using the [makemore]() library as a black box. \n",
    "\n",
    "Then we generate outputs with the language model and evaluate the outputs for correctness.\n",
    "\n",
    "The task is **4-digit addition**.\n",
    "\n",
    "\n",
    "------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 4990000 Test: 10000\n",
      "7272+3991=11263\n",
      "4576+3741=8317\n",
      "4180+3775=7955\n",
      "2503+3478=5981\n",
      "5642+1208=6850\n"
     ]
    }
   ],
   "source": [
    "!python generate_addition_data.py\n",
    "\n",
    "!head -n 5 data/addition_train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a transformer language model on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python makemore.py -i data/addition_train.txt --n-layer 8 --n-head 4 --n-embd 128 --n-embd2 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python makemore.py -i data/addition_train.txt --sample-only --n-layer 8 --n-head 4 --n-embd 128 --n-embd2 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate correctness\n",
    "\n",
    "Now we want to evaluate the correctness of the outputs. We'll give the model problems from the test set (which were not seen during training) and have the model generate a solution for each problem. Then we'll parse the output and check it.\n",
    "\n",
    "This will require writing some code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import makemore\n",
    "import torch\n",
    "\n",
    "def load(filename='out/model.pt', n_layer=8, n_head=4, n_embd=128, n_embd2=128):\n",
    "    train_dataset, test_dataset = makemore.create_datasets(\"data/addition_test.txt\")\n",
    "    vocab_size = train_dataset.get_vocab_size()\n",
    "    block_size = train_dataset.get_output_length()\n",
    "\n",
    "    config = makemore.ModelConfig(\n",
    "        vocab_size=vocab_size, block_size=block_size,\n",
    "        n_layer=n_layer, n_head=n_head,\n",
    "        n_embd=n_embd, n_embd2=n_embd2\n",
    "    )\n",
    "    model = makemore.Transformer(config)\n",
    "    model.load_state_dict(torch.load(filename, map_location=torch.device('cpu')))\n",
    "    return train_dataset, test_dataset, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_padding(x):\n",
    "    start = 0\n",
    "    end = len(x)\n",
    "    for j in range(len(x)):\n",
    "        if x[j] == 0:\n",
    "            start = j+1\n",
    "            break\n",
    "    for j in range(len(x)-1, start, -1):\n",
    "        if x[j] == 0:\n",
    "            end = j\n",
    "    x = x[start:end]\n",
    "    return x\n",
    "\n",
    "def check(train_dataset_decode, out):\n",
    "    out = out[0].tolist()\n",
    "    out = trim_padding(out)\n",
    "    out = train_dataset_decode(out)\n",
    "\n",
    "    # use a regex and evaluate (e.g. 1468+1657=3125)\n",
    "    import re\n",
    "    try:\n",
    "        m = re.match(r'(\\d+)\\+(\\d+)=(\\d+)', out)\n",
    "        a = int(m.group(1))\n",
    "        b = int(m.group(2))\n",
    "        c = int(m.group(3))\n",
    "        correct = (a + b) == c\n",
    "    except AttributeError:\n",
    "        a, b, c = -1, -1, -1\n",
    "        correct = False\n",
    "    return a, b, c, correct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate\n",
    "\n",
    "You can evaluate the model we trained for awhile or your own model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- To use our provided model, use:\n",
    "model_filename = 'out/model_provided.pt'\n",
    "\n",
    "# --- To use your model, use:\n",
    "# model_filename = 'out/model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples in the dataset: 10000\n",
      "max word length: 15\n",
      "number of unique characters in the vocabulary: 12\n",
      "vocabulary:\n",
      "+0123456789=\n",
      "split up the dataset into 9000 training examples and 1000 test examples\n",
      "number of parameters: 1.59M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:29<00:00, 33.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset, model = load(model_filename)\n",
    "train_dataset_decode = train_dataset.decode\n",
    "\n",
    "# Evaluate accuracy on the test set\n",
    "from tqdm import trange\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "dataset = test_dataset\n",
    "incorrect = []\n",
    "for i in trange(len(dataset)):\n",
    "    model.eval()\n",
    "    x = dataset[i][0].tolist()\n",
    "    x = trim_padding(x)\n",
    "    prompt = dataset.decode(x).split('=')[0]+'='\n",
    "    prompt_tokens = dataset.encode(prompt)\n",
    "    prompt_tokens = torch.cat([torch.tensor([0]), prompt_tokens]).unsqueeze(0)\n",
    "    out = makemore.generate(\n",
    "        model, prompt_tokens, 10, top_k=None, do_sample=False\n",
    "    ).to('cpu')\n",
    "    a, b, c, correct_ = check(train_dataset_decode, out)\n",
    "    correct += correct_\n",
    "    if not correct_:\n",
    "        incorrect.append((prompt, out, a, b, c))\n",
    "    total += 1\n",
    "print('Accuracy:', correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manually try it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2727+7272=9999'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"2727+7272=\"\n",
    "\n",
    "prompt_tokens = dataset.encode(prompt)\n",
    "prompt_tokens = torch.cat([torch.tensor([0]), prompt_tokens]).unsqueeze(0)\n",
    "out = makemore.generate(\n",
    "    model, prompt_tokens, 10, top_k=None, do_sample=False\n",
    ").to('cpu')\n",
    "out = out[0].tolist()\n",
    "out = trim_padding(out)\n",
    "out = train_dataset_decode(out)\n",
    "out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prototype",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
