{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/wellecks/transformers4math-simons/blob/main/3_addition/addition_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPfDOYknLbTh"
   },
   "source": [
    "### Part 3: Applications | Addition\n",
    "\n",
    "**Tutorial on Transformers for Mathematics**\n",
    "\n",
    "*Simons Institute and SLMath Joint Workshop: AI for Mathematics and Theoretical Computer Science, April 8 2025*\n",
    "\n",
    "Author: Sean Welleck\n",
    "\n",
    "------\n",
    "\n",
    "This notebook trains a transformer language model on a dataset using the [makemore]() library as a black box.\n",
    "\n",
    "Then we generate outputs with the language model and evaluate the outputs for correctness.\n",
    "\n",
    "The task is **4-digit addition**.\n",
    "\n",
    "\n",
    "------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tzGDM1RyLcvl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: out: File exists\n",
      "--2025-04-08 10:12:51--  https://raw.githubusercontent.com/wellecks/transformers4math-simons/refs/heads/main/3_addition/generate_addition_data.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8001::154, 2606:50c0:8002::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 989 [text/plain]\n",
      "Saving to: ‘generate_addition_data.py’\n",
      "\n",
      "generate_addition_d 100%[===================>]     989  --.-KB/s    in 0s      \n",
      "\n",
      "2025-04-08 10:12:51 (13.7 MB/s) - ‘generate_addition_data.py’ saved [989/989]\n",
      "\n",
      "--2025-04-08 10:12:51--  https://raw.githubusercontent.com/wellecks/transformers4math-simons/refs/heads/main/3_addition/makemore.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8001::154, 2606:50c0:8002::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 30176 (29K) [text/plain]\n",
      "Saving to: ‘makemore.py’\n",
      "\n",
      "makemore.py         100%[===================>]  29.47K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2025-04-08 10:12:51 (27.3 MB/s) - ‘makemore.py’ saved [30176/30176]\n",
      "\n",
      "--2025-04-08 10:12:51--  https://github.com/wellecks/transformers4math-simons/raw/refs/heads/main/3_addition/out/model_provided.pt\n",
      "Resolving github.com (github.com)... 140.82.116.3\n",
      "Connecting to github.com (github.com)|140.82.116.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/wellecks/transformers4math-simons/refs/heads/main/3_addition/out/model_provided.pt [following]\n",
      "--2025-04-08 10:12:52--  https://raw.githubusercontent.com/wellecks/transformers4math-simons/refs/heads/main/3_addition/out/model_provided.pt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8001::154, 2606:50c0:8002::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6406902 (6.1M) [application/octet-stream]\n",
      "Saving to: ‘out/model_provided.pt’\n",
      "\n",
      "out/model_provided. 100%[===================>]   6.11M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2025-04-08 10:12:53 (46.5 MB/s) - ‘out/model_provided.pt’ saved [6406902/6406902]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download files\n",
    "!mkdir out\n",
    "!wget https://raw.githubusercontent.com/wellecks/transformers4math-simons/refs/heads/main/3_addition/generate_addition_data.py -O generate_addition_data.py\n",
    "!wget https://raw.githubusercontent.com/wellecks/transformers4math-simons/refs/heads/main/3_addition/makemore.py -O makemore.py\n",
    "!wget https://github.com/wellecks/transformers4math-simons/raw/refs/heads/main/3_addition/out/model_provided.pt -O out/model_provided.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1qCNi9J3LbTh"
   },
   "source": [
    "#### Generate a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ovRRivNRLbTi",
    "outputId": "7c5e0d67-1f4d-4370-940e-65ca86c81c72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 4990000 Test: 10000\n",
      "7272+3991=11263\n",
      "4576+3741=8317\n",
      "4180+3775=7955\n",
      "2503+3478=5981\n",
      "5642+1208=6850\n"
     ]
    }
   ],
   "source": [
    "!python generate_addition_data.py\n",
    "\n",
    "!head -n 5 data/addition_train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZuzi9DGLbTi"
   },
   "source": [
    "#### Train a transformer language model on the dataset\n",
    "\n",
    "On Colab, please select go to `Runtime -> Change runtime type -> T4 GPU` to run this on a GPU. You'll need to re-run the cells up to this point. You can also run on CPU by removing `--device cuda` in the commands below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jnPM8TQbLbTi"
   },
   "outputs": [],
   "source": [
    "!python makemore.py -i data/addition_train.txt --n-layer 8 --n-head 4 --n-embd 128 --n-embd2 128 --device cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uq117QIjLbTi"
   },
   "source": [
    "#### Generate outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nfS871ScLbTi"
   },
   "outputs": [],
   "source": [
    "!python makemore.py -i data/addition_train.txt --sample-only --n-layer 8 --n-head 4 --n-embd 128 --n-embd2 128 --device cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXrrFd2ILbTi"
   },
   "source": [
    "### Evaluate correctness\n",
    "\n",
    "Now we want to evaluate the correctness of the outputs. We'll give the model problems from the test set (which were not seen during training) and have the model generate a solution for each problem. Then we'll parse the output and check it.\n",
    "\n",
    "This will require writing some code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s0rNfZIhLbTi"
   },
   "outputs": [],
   "source": [
    "import makemore\n",
    "import torch\n",
    "\n",
    "def load(filename='out/model.pt', n_layer=8, n_head=4, n_embd=128, n_embd2=128):\n",
    "    train_dataset, test_dataset = makemore.create_datasets(\"data/addition_test.txt\")\n",
    "    vocab_size = train_dataset.get_vocab_size()\n",
    "    block_size = train_dataset.get_output_length()\n",
    "\n",
    "    config = makemore.ModelConfig(\n",
    "        vocab_size=vocab_size, block_size=block_size,\n",
    "        n_layer=n_layer, n_head=n_head,\n",
    "        n_embd=n_embd, n_embd2=n_embd2\n",
    "    )\n",
    "    model = makemore.Transformer(config)\n",
    "    model.load_state_dict(torch.load(filename, map_location=torch.device('cpu')))\n",
    "    return train_dataset, test_dataset, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6llQze4ELbTi"
   },
   "outputs": [],
   "source": [
    "def trim_padding(x):\n",
    "    start = 0\n",
    "    end = len(x)\n",
    "    for j in range(len(x)):\n",
    "        if x[j] == 0:\n",
    "            start = j+1\n",
    "            break\n",
    "    for j in range(len(x)-1, start, -1):\n",
    "        if x[j] == 0:\n",
    "            end = j\n",
    "    x = x[start:end]\n",
    "    return x\n",
    "\n",
    "def check(train_dataset_decode, out):\n",
    "    out = out[0].tolist()\n",
    "    out = trim_padding(out)\n",
    "    out = train_dataset_decode(out)\n",
    "\n",
    "    # use a regex and evaluate (e.g. 1468+1657=3125)\n",
    "    import re\n",
    "    try:\n",
    "        m = re.match(r'(\\d+)\\+(\\d+)=(\\d+)', out)\n",
    "        a = int(m.group(1))\n",
    "        b = int(m.group(2))\n",
    "        c = int(m.group(3))\n",
    "        correct = (a + b) == c\n",
    "    except AttributeError:\n",
    "        a, b, c = -1, -1, -1\n",
    "        correct = False\n",
    "    return a, b, c, correct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBozOFl4LbTi"
   },
   "source": [
    "#### Evaluate\n",
    "\n",
    "You can evaluate the model we trained for awhile or your own model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h6xepfrkLbTi"
   },
   "outputs": [],
   "source": [
    "# --- To use our provided model, use:\n",
    "model_filename = 'out/model_provided.pt'\n",
    "\n",
    "# --- To use your model, use:\n",
    "# model_filename = 'out/model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DJrtQ2HnLbTi",
    "outputId": "e270dfa4-6d94-4da2-e923-84f213112589"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples in the dataset: 10000\n",
      "max word length: 15\n",
      "number of unique characters in the vocabulary: 12\n",
      "vocabulary:\n",
      "+0123456789=\n",
      "split up the dataset into 9000 training examples and 1000 test examples\n",
      "number of parameters: 1.59M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:54<00:00, 18.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset, model = load(model_filename)\n",
    "train_dataset_decode = train_dataset.decode\n",
    "\n",
    "# Evaluate accuracy on the test set\n",
    "from tqdm import trange\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "dataset = test_dataset\n",
    "incorrect = []\n",
    "for i in trange(len(dataset)):\n",
    "    model.eval()\n",
    "    x = dataset[i][0].tolist()\n",
    "    x = trim_padding(x)\n",
    "    prompt = dataset.decode(x).split('=')[0]+'='\n",
    "    prompt_tokens = dataset.encode(prompt)\n",
    "    prompt_tokens = torch.cat([torch.tensor([0]), prompt_tokens]).unsqueeze(0)\n",
    "    out = makemore.generate(\n",
    "        model, prompt_tokens, 10, top_k=None, do_sample=False\n",
    "    ).to('cpu')\n",
    "    a, b, c, correct_ = check(train_dataset_decode, out)\n",
    "    correct += correct_\n",
    "    if not correct_:\n",
    "        incorrect.append((prompt, out, a, b, c))\n",
    "    total += 1\n",
    "print('Accuracy:', correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0lyGBNbLbTj"
   },
   "source": [
    "#### Manually try it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "wBf1_kZ9LbTj",
    "outputId": "38f9b611-7fff-43c8-914c-2d0040124f47"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'2727+7272=9999'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"2727+7272=\"\n",
    "\n",
    "prompt_tokens = dataset.encode(prompt)\n",
    "prompt_tokens = torch.cat([torch.tensor([0]), prompt_tokens]).unsqueeze(0)\n",
    "out = makemore.generate(\n",
    "    model, prompt_tokens, 10, top_k=None, do_sample=False\n",
    ").to('cpu')\n",
    "out = out[0].tolist()\n",
    "out = trim_padding(out)\n",
    "out = train_dataset_decode(out)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VfhRH8_eNFJW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
